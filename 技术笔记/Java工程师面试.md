# Java工程师面试题

---

记录自己在面试过程中第一次没有答出来的题。方便自己持续学习和记录。（参考答案可能来自网络中其他博主的整理。）

## 抽象类和接口的区别

抽象类要被子类继承，接口要被类实现。

接口只能做方法声明，抽象类中可以作方法声明，也可以做方法实现。

接口里定义的变量只能是公共的静态的常量，抽象类中的变量是普通变量。

接口是设计的结果，抽象类是重构的结果。

抽象类和接口都是用来抽象具体对象的，但是接口的抽象级别最高。

抽象类可以有具体的方法和属性，接口只能有抽象方法和不可变常量。

抽象类主要用来抽象类别，接口主要用来抽象功能。

## CAS

CAS（Compare-and-Swap），即比较并替换，是一种实现并发算法时常用到的技术，Java并发包中的很多类都使用了CAS技术。属于乐观锁。CAS需要有3个操作数：内存地址V，旧的预期值A，即将要更新的目标值B。CAS指令执行时，当且仅当内存地址V的值与预期值A相等时，将内存地址V的值修改为B，否则就什么都不做。整个比较并替换的操作是一个原子操作。简单来说就是CPU去更新一个值，但如果想改的值不再是原来的值，操作就失败，因为很明显，有其它操作先改变了这个值。

当同步冲突出现的机会很少时，这种假设能带来较大的性能提升。

CAS虽然很高效的解决了原子操作问题，但是CAS仍然存在三大问题。

循环时间长开销很大。
只能保证一个变量的原子操作。
ABA问题。

循环时间长开销很大：
CAS 通常是配合无限循环一起使用的，我们可以看到 getAndAddInt 方法执行时，如果 CAS 失败，会一直进行尝试。如果 CAS 长时间一直不成功，可能会给 CPU 带来很大的开销。

 

只能保证一个变量的原子操作：
当对一个变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个变量操作时，CAS 目前无法直接保证操作的原子性。但是我们可以通过以下两种办法来解决：1）使用互斥锁来保证原子性；2）将多个变量封装成对象，通过 AtomicReference 来保证原子性。

 

什么是ABA问题？ABA问题怎么解决？
CAS 的使用流程通常如下：1）首先从地址 V 读取值 A；2）根据 A 计算目标值 B；3）通过 CAS 以原子的方式将地址 V 中的值从 A 修改为 B。

但是在第1步中读取的值是A，并且在第3步修改成功了，我们就能说它的值在第1步和第3步之间没有被其他线程改变过了吗？

如果在这段期间它的值曾经被改成了B，后来又被改回为A，那CAS操作就会误认为它从来没有被改变过。这个漏洞称为CAS操作的“ABA”问题。Java并发包为了解决这个问题，提供了一个带有标记的原子引用类“AtomicStampedReference”，它可以通过控制变量值的版本来保证CAS的正确性。因此，在使用CAS前要考虑清楚“ABA”问题是否会影响程序并发的正确性，如果需要解决ABA问题，改用传统的互斥同步可能会比原子类更高效。

## 乐观锁和悲观锁

**乐观锁与悲观锁是两种并发控制的思想，可用于解决丢失更新问题**

乐观锁会“乐观地”假定大概率不会发生并发更新冲突，访问、处理数据过程中不加锁，只在更新数据时再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务。用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式

悲观锁会“悲观地”假定大概率会发生并发更新冲突，访问、处理数据前就加排他锁，在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁。另外与乐观锁相对应的，**悲观锁是由数据库自己实现了的，要用的时候，我们直接调用数据库的相关语句就可以了。**

作者：贾不假
链接：https://juejin.im/post/6850037271233331208
来源：掘金

## 脏读、幻读、不可重复读

脏读(Dirty Reads)：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据

不可重复读（Non-Repeatable Reads)：事务 A 多次读取同一数据，事务B在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果不一致。

幻读（Phantom Reads)：幻读与不可重复读类似。它发生在一个事务A读取了几行数据，接着另一个并发事务B插入了一些数据时。在随后的查询中，事务A就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。


作者：贾不假
链接：https://juejin.im/post/6850037271233331208
来源：掘金

## 子类父类的加载过程

具体顺序如下:
1.父类静态变量
2.父类静态代码块（若有多个按代码先后顺序执行)
3.子类静态变量
4.子类静态代码块(若有多个按代码先后顺序执行)
5.父类非静态变量
6.父类非静态代码块（若有多个按代码先后顺序执行)
7.父类构造函数化
8.子类非静态变量
9.子类非静态代码块（若有多个按代码先后顺序执行)
10.子类构造函数

## Mysql的引擎

常见的存储引擎就 InnoDB、MyISAM、Memory、NDB。

InnoDB 现在是 MySQL 默认的存储引擎，支持事务、行级锁定和外键

1. InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；
2. InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败；
3. InnoDB 是聚簇索引，MyISAM 是非聚簇索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。
4. InnoDB 不保存表的具体行数，执行`select count(*) from table` 时需要全表扫描。而 MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快；
5. InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；

作者：贾不假
链接：https://juejin.im/post/6850037271233331208
来源：掘金

## 线程池

java.util.concurrent.Executors提供了一个 java.util.concurrent.Executor接口的实现用于创建线程池

线程池中的几种重要的参数及流程说明

corePoolSize：核心池的大小。

maximumPoolSize：线程池最大线程数。

keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。

unit：参数keepAliveTime的时间单位。

workQueue：一个阻塞队列，用来存储等待执行的任务

threadFactory：用于设置创建线程的工厂。

## 重排序什么时候会发生

重排序在多线程环境下出现的概率还是挺高的，在关键字上有volatile和synchronized可以禁用重排序，除此之外还有一些规则，也正是这些规则，使得我们在平时的编程工作中没有感受到重排序的坏处。
程序次序规则(Program Order Rule)：在一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确地说应该是控制流顺序而不是代码顺序，因为要考虑分支、循环等结构。
监视器锁定规则(Monitor Lock Rule)：一个unlock操作先行发生于后面对同一个对象锁的lock操作。这里强调的是同一个锁，而“后面”指的是时间上的先后顺序，如发生在其他线程中的lock操作。
volatile变量规则(Volatile Variable Rule):对一个volatile变量的写操作发生于后面对这个变量的读操作，这里的“后面”也指的是时间上的先后顺序。
线程启动规则(Thread Start Rule)：Thread独享的start()方法先行于此线程的每一个动作。
线程终止规则(Thread Termination Rule)：线程中的每个操作都先行发生于对此线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值检测到线程已经终止执行。
线程中断规则(Thread Interruption Rule)：对线程interrupte()方法的调用优先于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测线程是否已中断。
对象终结原则(Finalizer Rule)：一个对象的初始化完成(构造函数执行结束)先行发生于它的finalize()方法的开始。
传递性(Transitivity)：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。
正是以上这些规则保障了happen-before的顺序，如果不符合以上规则，那么在多线程环境下就不能保证执行顺序等同于代码顺序，也就是“如果在本线程中观察，所有的操作都是有序的；如果在一个线程中观察另外一个线程，则不符合以上规则的都是无序的”，因此，如果我们的多线程程序依赖于代码书写顺序，那么就要考虑是否符合以上规则，如果不符合就要通过一些机制使其符合，最常用的就是synchronized、Lock以及volatile修饰符。来源(https://www.cnblogs.com/xll1025/category/957062.html)

## ConcurrentHashMap

Concurrenthashmap线程安全的，1.7是在jdk1.7中采用Segment+HashEntry的方式进行实现的，lock加在Segment上面。1.7size计算是先采用不加锁的方式，连续计算元素的个数相同，最多计算3次:1、如果前后两次计算结果相同，则说明计算出来的元素个数是准确的;2、如果前后两次计算结果都不同，则给每个Segment进行加锁，再计算一次元素的个数;

1.8中放弃了Segment臃肿的设计，取而代之的是采用Node + CAS+ Synchronized来保证并发安全进行实现，1.8中使用一个volatile类型的变量baseCount记录元素的个数，当插入新数据或则删除数据时，会通过addCount()方法更新baseCount，通过累加baseCount和个数;CounterCell数组中的数量，即可得到元素的总个数;

## 类加载的过程

主要分为以下几个过程：加载、验证、准备、解析、初始化；
 加载：
 加载分为三步：
 1、通过类的全限定性类名获取该类的二进制流；
 2、将该二进制流的静态存储结构转为方法区的运行时数据结构；
 3、在堆中为该类生成一个class对象；

验证：
 验证该class文件中的字节流信息复合虚拟机的要求，不会威胁到jvm的安全；

准备：
 为class对象的静态变量分配内存，初始化其初始值；

解析：
 该阶段主要完成符号引用转化成直接引用；

初始化：
 到了初始化阶段，才开始执行类中定义的java代码；
 初始化阶段是调用类构造器的过程；

作者：前程有光
链接：https://www.jianshu.com/p/49e76079243d
来源：简书

## Hbase中的 “热点”问题

**hbase中的热点现象：**

我们知道，检索habse的记录首先要通过row key来定位数据行。当大量的client访问hbase集群的一个或少数几个节点，造成少数region server的读/写请求过多、负载过大，而其他region server负载却很小，就造成了“热点”现象。

 

**热点的危害：**

大量访问会使热点region所在的单个主机负载过大，引起性能下降甚至region不可用。

 

**热点产生原因：**

有大量连续编号的row key  ==>  大量row key相近的记录集中在个别region

 ==>  client检索记录时,对个别region访问过多  ==>  此region所在的主机过载

 ==>  热点

明白了热点原因就可以从row key着手解决，下面几个方法可以使用，目的就一个：尽量均衡地把每一条记录分散到不同的region里去！

 

**下面是一些常见的避免热点的方法以及它们的优缺点：**

加盐

​    这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。给多少个前缀？这个数量应该和我们想要分散数据到不同的region的数量一致（类似hive里面的分桶）。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。

 

哈希

​    哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。

 

反转

​    第三种防止热点的方法是反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。

反转rowkey的例子：以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，从而避免诸如139、158之类的固定号码开头导致的热点问题。

 

时间戳反转

​    一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用Long.Max_Value - timestamp追加到key的末尾，例如[key][reverse_timestamp] ,[key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。

 

尽量减少行和列的大小

​    在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，HBase storefiles中的索引（有助于随机访问）会占据HBase分配的大量内存，因为具体的值和它的key很大。可以增加block大小使得storefiles索引再更大的时间间隔增加，或者修改表的模式以减小rowkey和列名的大小。压缩也有助于更大的索引。

 

其他办法  

列族名的长度尽可能小，最好是只有一个字符。冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好。也可以在建表时预估数据规模，预留region数量，例如create 'myspace:mytable’, SPLITS => [01,02,03,,...99]

 

总结一下，row key的设计原则应当遵循以下几点

（1）rowkey唯一原则

必须在设计上保证其唯一性，rowkey是按照二进制字节数组排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。所以设计rwo key时尽量把体现业务特征的信息、业务上有唯一性的信息编进row key。

（2）rowkey长度原则

rowkey是一个二进制码流，可以是任意字符串，最大长度 64kb ，实际应用中一般为10-100byte，以byte[] 形式保存，一般设计成定长。建议越短越好，不要超过16个字节，2个原因——

原因1：

数据的持久化文件HFile中是按照(Key,Value)存储的，如果rowkey过长，例如超过100byte，那么1000万行的记录计算，仅row key就需占用100*1000万=10亿byte，近1Gb。这样会极大影响HFile的存储效率！

原因2：

MemStore将缓存部分数据到内存，若 rowkey字段过长，内存的有效利用率就会降低，就不能缓存更多的数据，从而降低检索效率。

目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性。

 

（3）rowkey散列原则

如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。

来源：https://blog.csdn.net/qq_31598113/article/details/71278857

## 扩容问题

ArrayList是1.5倍，HashMap是2倍。

## 有序Map

有TreeMap，可以根据Key值的大小排序，底层是红黑树。

有LinkedMap，可以根据放入的顺序排序，用了一个双向链表来记录顺序。

## 哈弗曼树

## 动态查找表